{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 files were found under current folder. \n",
      "Please be noted that only files end with '*.tfrecord' will be load!\n",
      "Cannot find any tfrecord files, please check the path.\n"
     ]
    }
   ],
   "source": [
    "import os  # handle system path and filenames\n",
    "import tensorflow as tf  # import tensorflow as usual\n",
    "\n",
    "# define a function to list tfrecord files.\n",
    "def list_tfrecord_file(file_list):\n",
    "    tfrecord_list = []\n",
    "    for i in range(len(file_list)):\n",
    "        current_file_abs_path = os.path.abspath(file_list[i])\t\t\n",
    "        if current_file_abs_path.endswith(\".tfrecord\"):\n",
    "            tfrecord_list.append(current_file_abs_path)\n",
    "            print(\"Found %s successfully!\" % file_list[i])\t\t\t\t\n",
    "        else:\n",
    "            pass\n",
    "    return tfrecord_list\n",
    "\t\n",
    "# Traverse current directory\n",
    "def tfrecord_auto_traversal():\n",
    "    current_folder_filename_list = os.listdir(r\"./\")\n",
    "    # Change this PATH to traverse other directories if you want.\n",
    "    if current_folder_filename_list != None:\n",
    "        print(\"%s files were found under current folder. \" % len(current_folder_filename_list))\n",
    "        print(\"Please be noted that only files end with '*.tfrecord' will be load!\")\n",
    "        tfrecord_list = list_tfrecord_file(current_folder_filename_list)\n",
    "        if len(tfrecord_list) != 0:\n",
    "            for list_index in xrange(len(tfrecord_list)):\n",
    "                print(tfrecord_list[list_index])\n",
    "        else:\n",
    "            print(\"Cannot find any tfrecord files, please check the path.\")\n",
    "    return tfrecord_list\n",
    "\n",
    "def main():\n",
    "    tfrecord_list = tfrecord_auto_traversal()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ####Delete all flags before declare#####\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to C:\\Users\\Dan Austin\\Documents\\GitHub\\Nannotaxi\n",
      "Determining list of input files and labels from C:\\Users\\Dan Austin\\Documents\\GitHub\\Nannotaxi.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "NewRandomAccessFile failed to Create/Open:  : The system cannot find the path specified.\r\n; No such process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-97bd76066b60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\geocomp\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-97bd76066b60>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(unused_argv)\u001b[0m\n\u001b[0;32m    402\u001b[0m   \u001b[1;31m# Run it!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m   _process_dataset('validation', FLAGS.validation_directory,\n\u001b[1;32m--> 404\u001b[1;33m                    FLAGS.validation_shards, FLAGS.labels_file)\n\u001b[0m\u001b[0;32m    405\u001b[0m   _process_dataset('train', FLAGS.train_directory,\n\u001b[0;32m    406\u001b[0m                    FLAGS.train_shards, FLAGS.labels_file)\n",
      "\u001b[1;32m<ipython-input-6-97bd76066b60>\u001b[0m in \u001b[0;36m_process_dataset\u001b[1;34m(name, directory, num_shards, labels_file)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[0mlabels_file\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m   \"\"\"\n\u001b[1;32m--> 390\u001b[1;33m   \u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_find_image_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m   \u001b[0m_process_image_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-97bd76066b60>\u001b[0m in \u001b[0;36m_find_image_files\u001b[1;34m(data_dir, labels_file)\u001b[0m\n\u001b[0;32m    341\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Determining list of input files and labels from %s.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m   unique_labels = [l.strip() for l in tf.gfile.FastGFile(\n\u001b[1;32m--> 343\u001b[1;33m       labels_file, 'r').readlines()]\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\geocomp\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mreadlines\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;34m\"\"\"Returns all lines from the file in a list.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\geocomp\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[1;32m---> 85\u001b[1;33m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Anaconda\\envs\\geocomp\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: NewRandomAccessFile failed to Create/Open:  : The system cannot find the path specified.\r\n; No such process"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Converts image data to TFRecords file format with Example protos.\n",
    "The image data set is expected to reside in JPEG files located in the\n",
    "following directory structure.\n",
    "  data_dir/label_0/image0.jpeg\n",
    "  data_dir/label_0/image1.jpg\n",
    "  ...\n",
    "  data_dir/label_1/weird-image.jpeg\n",
    "  data_dir/label_1/my-image.jpeg\n",
    "  ...\n",
    "where the sub-directory is the unique label associated with these images.\n",
    "This TensorFlow script converts the training and evaluation data into\n",
    "a sharded data set consisting of TFRecord files\n",
    "  train_directory/train-00000-of-01024\n",
    "  train_directory/train-00001-of-01024\n",
    "  ...\n",
    "  train_directory/train-01023-of-01024\n",
    "and\n",
    "  validation_directory/validation-00000-of-00128\n",
    "  validation_directory/validation-00001-of-00128\n",
    "  ...\n",
    "  validation_directory/validation-00127-of-00128\n",
    "where we have selected 1024 and 128 shards for each data set. Each record\n",
    "within the TFRecord file is a serialized Example proto. The Example proto\n",
    "contains the following fields:\n",
    "  image/encoded: string containing JPEG encoded image in RGB colorspace\n",
    "  image/height: integer, image height in pixels\n",
    "  image/width: integer, image width in pixels\n",
    "  image/colorspace: string, specifying the colorspace, always 'RGB'\n",
    "  image/channels: integer, specifying the number of channels, always 3\n",
    "  image/format: string, specifying the format, always 'JPEG'\n",
    "  image/filename: string containing the basename of the image file\n",
    "            e.g. 'n01440764_10026.JPEG' or 'ILSVRC2012_val_00000293.JPEG'\n",
    "  image/class/label: integer specifying the index in a classification layer.\n",
    "    The label ranges from [0, num_labels] where 0 is unused and left as\n",
    "    the background class.\n",
    "  image/class/text: string specifying the human-readable version of the label\n",
    "    e.g. 'dog'\n",
    "If your data set involves bounding boxes, please look at build_imagenet_data.py.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_directory', r'C:\\Users\\Dan Austin\\Documents\\GitHub\\Nannotaxi',\n",
    "                           'Training data directory')\n",
    "tf.app.flags.DEFINE_string('validation_directory', r'C:\\Users\\Dan Austin\\Documents\\GitHub\\Nannotaxi',\n",
    "                           'Validation data directory')\n",
    "tf.app.flags.DEFINE_string('output_directory', r'C:\\Users\\Dan Austin\\Documents\\GitHub\\Nannotaxi',\n",
    "                           'Output data directory')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('train_shards', 2,\n",
    "                            'Number of shards in training TFRecord files.')\n",
    "tf.app.flags.DEFINE_integer('validation_shards', 2,\n",
    "                            'Number of shards in validation TFRecord files.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_threads', 2,\n",
    "                            'Number of threads to preprocess the images.')\n",
    "\n",
    "# The labels file contains a list of valid labels are held in this file.\n",
    "# Assumes that the file contains entries as such:\n",
    "#   dog\n",
    "#   cat\n",
    "#   flower\n",
    "# where each line corresponds to a label. We map each label contained in\n",
    "# the file to an integer corresponding to the line number starting from 0.\n",
    "tf.app.flags.DEFINE_string('labels_file', '', 'Labels file')\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\"\n",
    "  if not isinstance(value, list):\n",
    "    value = [value]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Wrapper for inserting bytes features into Example proto.\"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _convert_to_example(filename, image_buffer, label, text, height, width):\n",
    "  \"\"\"Build an Example proto for an example.\n",
    "  Args:\n",
    "    filename: string, path to an image file, e.g., '/path/to/example.JPG'\n",
    "    image_buffer: string, JPEG encoding of RGB image\n",
    "    label: integer, identifier for the ground truth for the network\n",
    "    text: string, unique human-readable, e.g. 'dog'\n",
    "    height: integer, image height in pixels\n",
    "    width: integer, image width in pixels\n",
    "  Returns:\n",
    "    Example proto\n",
    "  \"\"\"\n",
    "\n",
    "  colorspace = 'RGB'\n",
    "  channels = 3\n",
    "  image_format = 'JPEG'\n",
    "\n",
    "  example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': _int64_feature(height),\n",
    "      'image/width': _int64_feature(width),\n",
    "      'image/colorspace': _bytes_feature(tf.compat.as_bytes(colorspace)),\n",
    "      'image/channels': _int64_feature(channels),\n",
    "      'image/class/label': _int64_feature(label),\n",
    "      'image/class/text': _bytes_feature(tf.compat.as_bytes(text)),\n",
    "      'image/format': _bytes_feature(tf.compat.as_bytes(image_format)),\n",
    "      'image/filename': _bytes_feature(tf.compat.as_bytes(os.path.basename(filename))),\n",
    "      'image/encoded': _bytes_feature(tf.compat.as_bytes(image_buffer))}))\n",
    "  return example\n",
    "\n",
    "\n",
    "class ImageCoder(object):\n",
    "  \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    # Create a single Session to run all image coding calls.\n",
    "    self._sess = tf.Session()\n",
    "\n",
    "    # Initializes function that converts PNG to JPEG data.\n",
    "    self._png_data = tf.placeholder(dtype=tf.string)\n",
    "    image = tf.image.decode_png(self._png_data, channels=3)\n",
    "    self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100)\n",
    "\n",
    "    # Initializes function that decodes RGB JPEG data.\n",
    "    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "  def png_to_jpeg(self, image_data):\n",
    "    return self._sess.run(self._png_to_jpeg,\n",
    "                          feed_dict={self._png_data: image_data})\n",
    "\n",
    "  def decode_jpeg(self, image_data):\n",
    "    image = self._sess.run(self._decode_jpeg,\n",
    "                           feed_dict={self._decode_jpeg_data: image_data})\n",
    "    assert len(image.shape) == 3\n",
    "    assert image.shape[2] == 3\n",
    "    return image\n",
    "\n",
    "\n",
    "def _is_png(filename):\n",
    "  \"\"\"Determine if a file contains a PNG format image.\n",
    "  Args:\n",
    "    filename: string, path of the image file.\n",
    "  Returns:\n",
    "    boolean indicating if the image is a PNG.\n",
    "  \"\"\"\n",
    "  return filename.endswith('.png')\n",
    "\n",
    "\n",
    "def _process_image(filename, coder):\n",
    "  \"\"\"Process a single image file.\n",
    "  Args:\n",
    "    filename: string, path to an image file e.g., '/path/to/example.JPG'.\n",
    "    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n",
    "  Returns:\n",
    "    image_buffer: string, JPEG encoding of RGB image.\n",
    "    height: integer, image height in pixels.\n",
    "    width: integer, image width in pixels.\n",
    "  \"\"\"\n",
    "  # Read the image file.\n",
    "  with tf.gfile.FastGFile(filename, 'rb') as f:\n",
    "    image_data = f.read()\n",
    "\n",
    "  # Convert any PNG to JPEG's for consistency.\n",
    "  if _is_png(filename):\n",
    "    print('Converting PNG to JPEG for %s' % filename)\n",
    "    image_data = coder.png_to_jpeg(image_data)\n",
    "\n",
    "  # Decode the RGB JPEG.\n",
    "  image = coder.decode_jpeg(image_data)\n",
    "\n",
    "  # Check that image converted to RGB\n",
    "  assert len(image.shape) == 3\n",
    "  height = image.shape[0]\n",
    "  width = image.shape[1]\n",
    "  assert image.shape[2] == 3\n",
    "\n",
    "  return image_data, height, width\n",
    "\n",
    "\n",
    "def _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n",
    "                               texts, labels, num_shards):\n",
    "  \"\"\"Processes and saves list of images as TFRecord in 1 thread.\n",
    "  Args:\n",
    "    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n",
    "    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n",
    "    ranges: list of pairs of integers specifying ranges of each batches to\n",
    "      analyze in parallel.\n",
    "    name: string, unique identifier specifying the data set\n",
    "    filenames: list of strings; each string is a path to an image file\n",
    "    texts: list of strings; each string is human readable, e.g. 'dog'\n",
    "    labels: list of integer; each integer identifies the ground truth\n",
    "    num_shards: integer number of shards for this data set.\n",
    "  \"\"\"\n",
    "  # Each thread produces N shards where N = int(num_shards / num_threads).\n",
    "  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n",
    "  # thread would produce shards [0, 64).\n",
    "  num_threads = len(ranges)\n",
    "  assert not num_shards % num_threads\n",
    "  num_shards_per_batch = int(num_shards / num_threads)\n",
    "\n",
    "  shard_ranges = np.linspace(ranges[thread_index][0],\n",
    "                             ranges[thread_index][1],\n",
    "                             num_shards_per_batch + 1).astype(int)\n",
    "  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n",
    "\n",
    "  counter = 0\n",
    "  for s in range(num_shards_per_batch):\n",
    "    # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'\n",
    "    shard = thread_index * num_shards_per_batch + s\n",
    "    output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)\n",
    "    output_file = os.path.join(FLAGS.output_directory, output_filename)\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "    shard_counter = 0\n",
    "    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n",
    "    for i in files_in_shard:\n",
    "      filename = filenames[i]\n",
    "      label = labels[i]\n",
    "      text = texts[i]\n",
    "\n",
    "      try:\n",
    "        image_buffer, height, width = _process_image(filename, coder)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        print('SKIPPED: Unexpected error while decoding %s.' % filename)\n",
    "        continue\n",
    "\n",
    "      example = _convert_to_example(filename, image_buffer, label,\n",
    "                                    text, height, width)\n",
    "      writer.write(example.SerializeToString())\n",
    "      shard_counter += 1\n",
    "      counter += 1\n",
    "\n",
    "      if not counter % 1000:\n",
    "        print('%s [thread %d]: Processed %d of %d images in thread batch.' %\n",
    "              (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    writer.close()\n",
    "    print('%s [thread %d]: Wrote %d images to %s' %\n",
    "          (datetime.now(), thread_index, shard_counter, output_file))\n",
    "    sys.stdout.flush()\n",
    "    shard_counter = 0\n",
    "  print('%s [thread %d]: Wrote %d images to %d shards.' %\n",
    "        (datetime.now(), thread_index, counter, num_files_in_thread))\n",
    "  sys.stdout.flush()\n",
    "\n",
    "\n",
    "def _process_image_files(name, filenames, texts, labels, num_shards):\n",
    "  \"\"\"Process and save list of images as TFRecord of Example protos.\n",
    "  Args:\n",
    "    name: string, unique identifier specifying the data set\n",
    "    filenames: list of strings; each string is a path to an image file\n",
    "    texts: list of strings; each string is human readable, e.g. 'dog'\n",
    "    labels: list of integer; each integer identifies the ground truth\n",
    "    num_shards: integer number of shards for this data set.\n",
    "  \"\"\"\n",
    "  assert len(filenames) == len(texts)\n",
    "  assert len(filenames) == len(labels)\n",
    "\n",
    "  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n",
    "  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n",
    "  ranges = []\n",
    "  for i in range(len(spacing) - 1):\n",
    "    ranges.append([spacing[i], spacing[i + 1]])\n",
    "\n",
    "  # Launch a thread for each batch.\n",
    "  print('Launching %d threads for spacings: %s' % (FLAGS.num_threads, ranges))\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  # Create a mechanism for monitoring when all threads are finished.\n",
    "  coord = tf.train.Coordinator()\n",
    "\n",
    "  # Create a generic TensorFlow-based utility for converting all image codings.\n",
    "  coder = ImageCoder()\n",
    "\n",
    "  threads = []\n",
    "  for thread_index in range(len(ranges)):\n",
    "    args = (coder, thread_index, ranges, name, filenames,\n",
    "            texts, labels, num_shards)\n",
    "    t = threading.Thread(target=_process_image_files_batch, args=args)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "  # Wait for all the threads to terminate.\n",
    "  coord.join(threads)\n",
    "  print('%s: Finished writing all %d images in data set.' %\n",
    "        (datetime.now(), len(filenames)))\n",
    "  sys.stdout.flush()\n",
    "\n",
    "\n",
    "def _find_image_files(data_dir, labels_file):\n",
    "  \"\"\"Build a list of all images files and labels in the data set.\n",
    "  Args:\n",
    "    data_dir: string, path to the root directory of images.\n",
    "      Assumes that the image data set resides in JPEG files located in\n",
    "      the following directory structure.\n",
    "        data_dir/dog/another-image.JPEG\n",
    "        data_dir/dog/my-image.jpg\n",
    "      where 'dog' is the label associated with these images.\n",
    "    labels_file: string, path to the labels file.\n",
    "      The list of valid labels are held in this file. Assumes that the file\n",
    "      contains entries as such:\n",
    "        dog\n",
    "        cat\n",
    "        flower\n",
    "      where each line corresponds to a label. We map each label contained in\n",
    "      the file to an integer starting with the integer 0 corresponding to the\n",
    "      label contained in the first line.\n",
    "  Returns:\n",
    "    filenames: list of strings; each string is a path to an image file.\n",
    "    texts: list of strings; each string is the class, e.g. 'dog'\n",
    "    labels: list of integer; each integer identifies the ground truth.\n",
    "  \"\"\"\n",
    "  print('Determining list of input files and labels from %s.' % data_dir)\n",
    "  unique_labels = [l.strip() for l in tf.gfile.FastGFile(\n",
    "      labels_file, 'r').readlines()]\n",
    "\n",
    "  labels = []\n",
    "  filenames = []\n",
    "  texts = []\n",
    "\n",
    "  # Leave label index 0 empty as a background class.\n",
    "  label_index = 1\n",
    "\n",
    "  # Construct the list of JPEG files and labels.\n",
    "  for text in unique_labels:\n",
    "    jpeg_file_path = '%s/%s/*' % (data_dir, text)\n",
    "    matching_files = tf.gfile.Glob(jpeg_file_path)\n",
    "\n",
    "    labels.extend([label_index] * len(matching_files))\n",
    "    texts.extend([text] * len(matching_files))\n",
    "    filenames.extend(matching_files)\n",
    "\n",
    "    if not label_index % 100:\n",
    "      print('Finished finding files in %d of %d classes.' % (\n",
    "          label_index, len(labels)))\n",
    "    label_index += 1\n",
    "\n",
    "  # Shuffle the ordering of all image files in order to guarantee\n",
    "  # random ordering of the images with respect to label in the\n",
    "  # saved TFRecord files. Make the randomization repeatable.\n",
    "  shuffled_index = list(range(len(filenames)))\n",
    "  random.seed(12345)\n",
    "  random.shuffle(shuffled_index)\n",
    "\n",
    "  filenames = [filenames[i] for i in shuffled_index]\n",
    "  texts = [texts[i] for i in shuffled_index]\n",
    "  labels = [labels[i] for i in shuffled_index]\n",
    "\n",
    "  print('Found %d JPEG files across %d labels inside %s.' %\n",
    "        (len(filenames), len(unique_labels), data_dir))\n",
    "  return filenames, texts, labels\n",
    "\n",
    "\n",
    "def _process_dataset(name, directory, num_shards, labels_file):\n",
    "  \"\"\"Process a complete data set and save it as a TFRecord.\n",
    "  Args:\n",
    "    name: string, unique identifier specifying the data set.\n",
    "    directory: string, root path to the data set.\n",
    "    num_shards: integer number of shards for this data set.\n",
    "    labels_file: string, path to the labels file.\n",
    "  \"\"\"\n",
    "  filenames, texts, labels = _find_image_files(directory, labels_file)\n",
    "  _process_image_files(name, filenames, texts, labels, num_shards)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  assert not FLAGS.train_shards % FLAGS.num_threads, (\n",
    "      'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards')\n",
    "  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n",
    "      'Please make the FLAGS.num_threads commensurate with '\n",
    "      'FLAGS.validation_shards')\n",
    "  print('Saving results to %s' % FLAGS.output_directory)\n",
    "\n",
    "  # Run it!\n",
    "  _process_dataset('validation', FLAGS.validation_directory,\n",
    "                   FLAGS.validation_shards, FLAGS.labels_file)\n",
    "  _process_dataset('train', FLAGS.train_directory,\n",
    "                   FLAGS.train_shards, FLAGS.labels_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocomp",
   "language": "python",
   "name": "geocomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
